{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks \n",
    "(Python Solution to get an overview of the problem)\n",
    "1. Extract the Embeddings of Documents & Query efficiently (In-/Out-Embeddings > 5GB each!)\n",
    "2. Visualize the document-query embedding space\n",
    "3. Compute DESM-Score and evaluate with experiment of Paper A Dual Embedding Space Model for Document Ranking (Table 2 in paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "1. word embeddings (in.txt, out.txt) need to be in same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Cambridge is a university city and the county town of Cambridgeshire, England.  It lies\\nin East Anglia, on the River Cam, about 50 miles (80 km) north of London. According to the United\\nKingdom Census 2011, its population was 123,867 (including 24,488 students). This makes Cambridge\\nthe second largest city in Cambridgeshire after Peterborough, and the 54th largest in the United Kingdom.\\nThere is archaeological evidence of settlement in the area during the Bronze Age and Roman times;\\nunder Viking rule Cambridge became an important trading centre. The first town charters were granted\\nin the 12th century, although city status was not conferred until 1951.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show first Document of the input-data\n",
    "open('data.txt').read().split('\\n\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize Documents (documents are seperated by \"\\n\\n\" and terminated by \"\\n\\n\\n\")\n",
    "def tokenize_docs(path, amountDocs):\n",
    "    #get documents (as a string) into a list\n",
    "    f = open(path)\n",
    "    docs = f.read().split('\\n\\n')[:amountDocs]\n",
    "    f.close()\n",
    "    #Tokenize documents\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    doc_list = []\n",
    "    for idx, doc in enumerate(docs):\n",
    "        doc = doc.lower()\n",
    "        doc_list.append(tokenizer.tokenize(doc))\n",
    "    #format numbers correctly for each document\n",
    "    list_numb = []\n",
    "    next_numb = False\n",
    "    for doc in doc_list:\n",
    "        numb = []\n",
    "        for idx, word in enumerate(doc):\n",
    "            if idx<len(doc)-1 and doc[idx].isdigit() and doc[idx+1].isdigit():\n",
    "                numb.append(doc[idx] + '.' + doc[idx+1])\n",
    "                next_numb = True\n",
    "            elif next_numb:\n",
    "                next_numb = False\n",
    "                continue\n",
    "            else:\n",
    "                numb.append(word)\n",
    "        list_numb.append(numb)\n",
    "    return list_numb #doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of the tokenized documents\n",
    "documents = tokenize_docs('data.txt', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build an offset index for each word in the embedding-file to later access the correct word-embedding using seek()\n",
    "def build_embed_index(path):\n",
    "    f = open(path)\n",
    "    d = dict()\n",
    "    offset = 0\n",
    "    for idx, word in enumerate(f):\n",
    "        if idx%1000000==0:\n",
    "            print('Progress: Line ' + str(idx))\n",
    "        d[word.split('\\t')[0]] = offset\n",
    "        offset += len(word)+1\n",
    "    f.close()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: Line 0\n"
     ]
    }
   ],
   "source": [
    "#Build index for in & out word-embeddings\n",
    "in_dic = build_embed_index('../../model/in.txt')\n",
    "out_dic = build_embed_index('../../model/out.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As an example: Show the in-word-embedding for the word \"the\"\n",
    "f = open('../../model/in.txt')\n",
    "f.seek(0)\n",
    "f.seek(in_dic['the'])\n",
    "w = f.readline()\n",
    "print(w.split('\\t')[0], [float(i) for i in w.split('\\t')[1:]])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the word-vectors for each word in a document, yielding a list of word-vectors instead of the word itself per document\n",
    "def extract_embed(file, dic, doc):\n",
    "    fd = open(file)\n",
    "    vecs = []\n",
    "    for word in doc:\n",
    "        if word not in dic.keys():\n",
    "            continue\n",
    "            #vecs.append(np.zeros(200))\n",
    "        else:\n",
    "            fd.seek(dic[word])\n",
    "            w = fd.readline()\n",
    "            vecs.append(np.array([float(i) for i in w.split('\\t')[1:]]))\n",
    "    fd.close()\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get word embeddings for each document, yielding the Data Structure: doc_embeds[doc][vec][item]\n",
    "doc_embeds = []\n",
    "for document in documents:\n",
    "    doc_embeds.append(extract_embed('../../model/out.txt', out_dic, document))\n",
    "#To test the result, print the length of the first word vector of the first document (each vector should have 200 entries)\n",
    "len(doc_embeds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get word embeddings for the query, resulting Data Structure: query_embeds[query][vec][item]\n",
    "query = ['cambridge']\n",
    "query_embeds = []\n",
    "query_embeds.append(extract_embed('../in.txt', in_dic, query))\n",
    "#To test the result, print the length of the first word vector of the first query (each vector should have 200 entries)\n",
    "len(query_embeds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using gensim to load the word embeddings doesn't seem to work because of formating (also the files are really big)\n",
    "# =>correct formating is another approach to try!\n",
    "#wordvecs = gensim.models.KeyedVectors.load_word2vec_format('in.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize document-query embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to represent a Document D with word embeddings\n",
    "Here D is the centroid of all the normalized vectors for the words in the document serving as a single embedding for the whole document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute a centroid of word embeddings\n",
    "To find the centroid, one computes the (arithmetic) mean of the points' positions separately for each dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing a Documents-Query Representation with PCA\n",
    "A  two  dimensional  PCA  projection  of  the  200-\n",
    "dimensional embeddings. Relevant documents are yellow, irrel-\n",
    "evant documents are grey, and the query is blue.  To visualize\n",
    "the  results  of  multiple  queries  at  once,  before  dimensionality\n",
    "reduction we centre query vectors at the origin and represent\n",
    "documents as the difference between the document vector and\n",
    "its query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Document-Centroid\n",
    "\n",
    "#A function to compute the euclidean norm of a vector\n",
    "def euclid_norm(v):\n",
    "    norm = 0\n",
    "    for i in v:\n",
    "        norm += i**2\n",
    "    return np.sqrt(norm)\n",
    "\n",
    "#A function to compute the absolute/manhattan norm of a vector\n",
    "def abs_norm(v):\n",
    "    norm = 0\n",
    "    for i in v:\n",
    "        norm += abs(i)\n",
    "    return norm\n",
    "\n",
    "#Compute the centroid of the word-vectors of a Document to get a Document-Vector\n",
    "def compute_centroid(D):\n",
    "    D_N = len(D)\n",
    "    centroid = np.zeros(len(D[0]))\n",
    "    for d in D:\n",
    "        centroid += d / euclid_norm(d)\n",
    "    return centroid/D_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the document vectors by computing the centroid of the word-vectors of the document, yielding the data structure: doc_centroids[doc][entry]\n",
    "doc_centroids = []\n",
    "for embed in doc_embeds:\n",
    "    doc_centroids.append(compute_centroid(embed))\n",
    "#To test the result, print the length of the first document vector (each vector should have 200 entries)\n",
    "len(doc_centroids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_centroids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e19825398806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create a PCA-Object to project the 200-dim. Document-Vector into a 2-dim. Vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoc_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_centroids\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery_embeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_centroids' is not defined"
     ]
    }
   ],
   "source": [
    "#Create a PCA-Object to project the 200-dim. Document-Vector into a 2-dim. Vector\n",
    "pca = PCA(n_components=2)\n",
    "doc_pca = pca.fit_transform(doc_centroids + query_embeds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-82fdbbeb245c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Now we can visualize the Documents in a 2-dim. Space, for human interpretability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_pca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_pca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdoc_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Cambridge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Oxford'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Giraffes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Giraffes_switched'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Cambridge_switched'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_pca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_pca\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_pca' is not defined"
     ]
    }
   ],
   "source": [
    "#Now we can visualize the Documents in a 2-dim. Space, for human interpretability\n",
    "pyplot.scatter(list(doc_pca[:, 0]), list(doc_pca[:, 1]))\n",
    "doc_title = ['Cambridge', 'Oxford', 'Giraffes', 'Giraffes_switched', 'Cambridge_switched', 'query']\n",
    "for idx, doc in enumerate(doc_title):\n",
    "    pyplot.annotate(doc, xy=(doc_pca[idx,0], doc_pca[idx,1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute DESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute DESM-Measure for a query and all documents (see paper for the DESM-formula)\n",
    "def desm(Q, D):\n",
    "    #Denominator\n",
    "    Q_N = len(Q)\n",
    "    #Sum of cosine similarities between query term and document\n",
    "    Q_cosine = 0\n",
    "    for q in Q:\n",
    "        Q_cosine += q.dot(D) / euclid_norm(q) * euclid_norm(D)\n",
    "    return Q_cosine/Q_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cambridge', -0.05113931496738447),\n",
       " ('Oxford', -0.05702753890771936),\n",
       " ('Cambridge_switched', -0.06067280038670073),\n",
       " ('Giraffes_switched', -0.07643237307293758),\n",
       " ('Giraffes', -0.0825272178554687)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Helper-Function for the sorting of the DESM-Scores\n",
    "def sort_help(elem):\n",
    "    return elem[1]\n",
    "\n",
    "#Create list of tuples for each Document-Title & the DESM-Score of the Query & the Document\n",
    "ranking = [(doc_title[idx], desm(query_embeds[0], doc_centroids[idx])) for idx in range(len(doc_centroids))]\n",
    "\n",
    "#Sort the DESM-Score, to show the most relevant document on top of the list\n",
    "ranking.sort(key=sort_help, reverse=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that for the query \"cambridge\" the document about cambridge is the most relevant.\n",
    "This is followed by the document about oxford, which is also an article about a univerity.\n",
    "More interestingly, the article about cambridge, where the word \"cambridge\" was substituted by the word \"giraffe\" is next relevant even though it doesn't contains the word cambridge, while the document about giraffes where the word \"giraffe\" is substituded by the word \"cambridge\" has a much worse score. This makes sense because the document is about giraffes and not about cambridge. Hence word-stuffing-trick has way less impact on the DESM-ranking then it would have on a TFIDF-Ranking (because it would count the word cambridge which would give it much more relevance).\n",
    "As expected, the document about Giraffes has the least relevance in the ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
