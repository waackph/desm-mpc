{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks \n",
    "(Python Solution to get an overview of the problem)\n",
    "1. Extract the Embeddings of Documents & Query efficiently (In-/Out-Embeddings > 5GB each!)\n",
    "2. Visualize the document-query embedding space\n",
    "3. Compute DESM-Score and evaluate with experiment of Paper A Dual Embedding Space Model for Document Ranking (Table 2 in paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "1. word embeddings (in.txt, out.txt) need to be in same directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city of Cambridge is a university city and the county town of Cambridgeshire, England.  It lies\\nin East Anglia, on the River Cam, about 50 miles (80 km) north of London. According to the United\\nKingdom Census 2011, its population was 123,867 (including 24,488 students). This makes Cambridge\\nthe second largest city in Cambridgeshire after Peterborough, and the 54th largest in the United Kingdom.\\nThere is archaeological evidence of settlement in the area during the Bronze Age and Roman times;\\nunder Viking rule Cambridge became an important trading centre. The first town charters were granted\\nin the 12th century, although city status was not conferred until 1951.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show first Document of the input-data\n",
    "open('../../data/data.txt').read().split('\\n\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize Documents (documents are seperated by \"\\n\\n\" and terminated by \"\\n\\n\\n\")\n",
    "def tokenize_docs(path, amountDocs):\n",
    "    #get documents (as a string) into a list\n",
    "    f = open(path)\n",
    "    docs = f.read().split('\\n\\n')[:amountDocs]\n",
    "    f.close()\n",
    "    #Tokenize documents\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    doc_list = []\n",
    "    for idx, doc in enumerate(docs):\n",
    "        doc = doc.lower()\n",
    "        doc_list.append(tokenizer.tokenize(doc))\n",
    "    #format numbers correctly for each document\n",
    "    list_numb = []\n",
    "    next_numb = False\n",
    "    for doc in doc_list:\n",
    "        numb = []\n",
    "        for idx, word in enumerate(doc):\n",
    "            if idx<len(doc)-1 and doc[idx].isdigit() and doc[idx+1].isdigit():\n",
    "                numb.append(doc[idx] + '.' + doc[idx+1])\n",
    "                next_numb = True\n",
    "            elif next_numb:\n",
    "                next_numb = False\n",
    "                continue\n",
    "            else:\n",
    "                numb.append(word)\n",
    "        list_numb.append(numb)\n",
    "    return list_numb #doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of the tokenized documents\n",
    "documents = tokenize_docs('../../data/data.txt', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build an offset index for each word in the embedding-file to later access the correct word-embedding using seek()\n",
    "def build_embed_index(path):\n",
    "    f = open(path)\n",
    "    d = dict()\n",
    "    offset = 0\n",
    "    for idx, word in enumerate(f):\n",
    "        if idx%1000000==0:\n",
    "            print('Progress: Line ' + str(idx))\n",
    "        d[word.split('\\t')[0]] = offset\n",
    "        offset += len(word)+1\n",
    "    f.close()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_modelfile = '../../model/in.txt'\n",
    "out_modelfile = '../../model/out.txt'\n",
    "#dt = np.dtype(\"U, (200,)f8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build index for in & out word-embeddings\n",
    "#in_dic = build_embed_index(in_modelfile)\n",
    "#out_dic = build_embed_index(out_modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model indices for quick access (without loading model into memory)\n",
    "\"\"\"\n",
    "with open('/home/phil/Schreibtisch/Uni/MA/Forschungsprojekt/in_index.obj', 'wb') as f:\n",
    "    pickle.dump(in_dic, f)\n",
    "\n",
    "with open('/home/phil/Schreibtisch/Uni/MA/Forschungsprojekt/out_index.obj', 'wb') as f:\n",
    "    pickle.dump(out_dic, f)\n",
    "\"\"\"\n",
    "\n",
    "# load model indices\n",
    "with open('../../model/in_index.obj', 'rb') as f:\n",
    "    in_dic = pickle.load(f)\n",
    "\n",
    "with open('../../model/out_index.obj', 'rb') as f:\n",
    "    out_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_int = 0\n",
    "with open(in_modelfile, 'r') as modelfile:\n",
    "    with open(\"model_new_tst\", 'w') as newmodel:\n",
    "        newmodel.write(\"101 200\\n\")\n",
    "        for line in modelfile:\n",
    "            break_int += 1\n",
    "            newmodel.write(line)\n",
    "            if break_int == 100:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-767b30d2c9e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_new_tst\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec()\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(\"model_new_tst\", binary=False, datatype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cambridge [0.04458, -0.008986, -0.060948, -0.017803, 0.043617, 0.059742, 0.016003, 0.047541, 0.020506, 0.050478, -0.025849, -0.01772, -0.049592, -0.010154, 0.060938, 0.145544, -0.05914, 0.046458, -0.057872, 0.063647, -0.040152, 0.019553, 0.072412, 0.016585, -0.06485, 0.07392, -0.023465, 0.10323, 0.033276, 0.046891, -0.06604, 0.089709, -0.013042, 0.023901, -0.088856, -0.116092, -0.051974, 0.028408, -0.042594, -0.066716, -0.083513, 0.021399, 0.054435, -0.057351, -0.04807, 0.09534, 0.054126, -0.042042, -0.006303, -0.021994, 0.102618, 0.121487, -0.032814, -0.066175, 0.071879, 0.050986, 0.015519, 0.101778, -0.017493, -0.0082, 0.11973, -0.006143, -0.134806, -0.030183, -0.099897, -0.054662, 0.030468, 0.053002, 0.052606, -0.003097, -0.079577, 0.055978, 0.034838, 0.036461, -0.056506, 0.001546, 0.023694, -0.061339, -0.030392, 0.028833, 0.044646, 0.044805, 0.035043, 0.089791, -0.041356, -0.034887, 0.022974, -0.025776, 0.011678, -0.048851, 0.050003, 0.075171, -0.077506, 0.061281, 0.018234, 0.034924, -0.054486, -0.023148, 0.04967, -0.037797, 0.004899, 0.059524, 0.051093, 0.060388, 0.077362, 0.119049, -0.003686, -0.078293, 0.024893, -0.028049, -0.012109, -0.068692, 0.108549, -0.072499, 0.011194, 0.106088, 0.020271, 0.064712, -0.011894, -0.040882, -0.118573, 0.051815, 0.042526, 0.032066, 0.186122, 0.027743, 0.06652, 0.105785, -0.024183, -0.014505, -0.002989, 0.085211, 0.152362, 0.089936, 0.007371, 0.135352, -0.131279, -0.181871, 0.065822, 0.05445, 0.206425, 0.025386, 0.000896, 0.013236, -0.009589, 0.072919, -0.061631, 0.087747, -0.105101, -0.114079, -0.024143, 0.011615, -0.032534, -0.094159, 0.125836, -0.158895, -0.015744, 0.039135, -0.085246, 0.075489, 0.080253, 0.075218, -0.090769, 0.130184, 0.022855, 0.083172, 0.034794, 0.0518, 0.066524, -0.004649, -0.035322, 0.113644, -0.037286, -0.00264, -0.001534, -0.052365, 0.026423, 0.018602, -0.01582, 0.016643, -0.153259, 0.060309, -0.09268, -0.029055, 0.087337, -0.07463, 0.052572, 0.121729, 0.037575, 0.086311, -0.080767, 0.065263, 0.051773, -0.116826, 0.186747, -0.075001, -0.037817, 0.08585, -0.095529, 0.126134]\n"
     ]
    }
   ],
   "source": [
    "#As an example: Show the in-word-embedding for the word \"the\"\n",
    "f = open('../../model/in.txt')\n",
    "f.seek(0)\n",
    "f.seek(in_dic['cambridge'])\n",
    "w = f.readline()\n",
    "print(w.split('\\t')[0], [float(i) for i in w.split('\\t')[1:]])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-0.075592\t0.184428\t0.046158\t-0.009972\t-0.074386\t-0.066087\t-0.029322\t-0.068233\t0.110443\t0.053467\t0.003435\t-0.107332\t0.046287\t-0.058982\t-0.013866\t-0.193059\t0.009037\t0.013276\t0.004489\t0.008422\t-0.014765\t-0.0909\t0.077934\t0.026567\t0.024279\t0.011647\t0.034532\t0.011656\t0.043914\t-0.083553\t-0.058438\t-0.03485\t-0.00556\t-0.089509\t0.129058\t-0.037916\t0.097282\t-0.096069\t-0.015938\t-0.142527\t0.037979\t-0.021787\t-0.016461\t0.04397\t-0.045817\t0.048476\t-0.081132\t0.059937\t-0.035035\t0.029006\t-0.010826\t0.006187\t0.084474\t0.020033\t0.071442\t-0.026707\t0.02595\t0.028096\t0.034807\t0.07139\t0.081373\t0.102673\t-0.145274\t0.044192\t-0.052757\t0.067243\t0.014967\t-0.06484\t-0.029073\t-0.020838\t-0.08279\t0.021637\t0.04848\t0.021208\t-0.046679\t-0.083477\t0.021796\t-0.014\t0.009601\t-0.000605\t0.086996\t-0.018368\t0.057951\t0.049502\t0.001449\t-0.081139\t-0.075676\t0.015095\t0.005328\t0.20621\t0.021733\t0.019247\t-0.05793\t0.041402\t-0.107244\t-0.055113\t0.041635\t-0.035302\t-0.029025\t-0.048554\t0.019056\t-0.001673\t0.013894\t-0.115121\t-0.07555\t0.086991\t-0.06738\t0.138175\t-0.021534\t0.084558\t-0.13377\t-0.029019\t-0.02372\t0.000171\t0.101744\t0.088861\t-0.047669\t-0.035363\t0.037897\t0.063505\t0.062802\t0.041121\t0.020104\t-0.061119\t0.041696\t0.117542\t0.003544\t-0.185422\t-0.005663\t0.024284\t0.087484\t-0.018431\t0.048144\t-0.168587\t0.004755\t-0.042475\t0.021368\t0.026837\t-0.086125\t-0.07098\t0.112323\t-0.023781\t-0.040957\t0.113357\t-0.022651\t-0.042598\t0.034259\t0.011424\t-0.021773\t-0.031273\t0.112121\t-0.135966\t0.056553\t0.097776\t-0.021462\t0.026931\t0.020071\t-0.063486\t-0.01968\t-0.039771\t-0.047898\t0.037767\t-0.126302\t0.134929\t-0.232857\t-0.123792\t0.02752\t0.013311\t0.006281\t0.030963\t-0.004954\t0.032921\t0.004519\t-0.112881\t0.048355\t-0.026726\t0.056434\t-0.048009\t-0.037388\t-0.010004\t0.093625\t-0.032767\t0.044021\t-0.071228\t-0.049562\t-0.030824\t0.03968\t-0.18082\t0.113663\t-0.019539\t-0.024301\t0.07386\t0.048592\t0.166392\t-0.027737\t0.083514\t-0.085165\t0.001186\t0.053182\t-0.083249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(in_modelfile, 'r') as modelfile:\n",
    "    print(modelfile.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the word-vectors for each word in a document, yielding a list of word-vectors instead of the word itself per document\n",
    "def extract_embed(file, dic, doc):\n",
    "    fd = open(file)\n",
    "    vecs = []\n",
    "    for word in doc:\n",
    "        if word not in dic.keys():\n",
    "            continue\n",
    "            #vecs.append(np.zeros(200))\n",
    "        else:\n",
    "            fd.seek(dic[word])\n",
    "            w = fd.readline()\n",
    "            vecs.append(np.array([float(i) for i in w.split('\\t')[1:]]))\n",
    "    fd.close()\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word embeddings for each document, yielding the Data Structure: doc_embeds[doc][vec][item]\n",
    "doc_embeds = []\n",
    "for document in documents:\n",
    "    doc_embeds.append(extract_embed('../../model/out.txt', out_dic, document))\n",
    "#To test the result, print the length of the first word vector of the first document (each vector should have 200 entries)\n",
    "len(doc_embeds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word embeddings for the query, resulting Data Structure: query_embeds[query][vec][item]\n",
    "query = 'cambridge city'\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "q_tokens = tokenizer.tokenize(query)\n",
    "\n",
    "query_embeds = []\n",
    "query_embeds.append(extract_embed('../../model/in.txt', in_dic, q_tokens))\n",
    "#To test the result, print the length of the first word vector of the first query (each vector should have 200 entries)\n",
    "len(query_embeds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04458 , -0.008986, -0.060948, -0.017803,  0.043617,  0.059742,\n",
       "        0.016003,  0.047541,  0.020506,  0.050478, -0.025849, -0.01772 ,\n",
       "       -0.049592, -0.010154,  0.060938,  0.145544, -0.05914 ,  0.046458,\n",
       "       -0.057872,  0.063647, -0.040152,  0.019553,  0.072412,  0.016585,\n",
       "       -0.06485 ,  0.07392 , -0.023465,  0.10323 ,  0.033276,  0.046891,\n",
       "       -0.06604 ,  0.089709, -0.013042,  0.023901, -0.088856, -0.116092,\n",
       "       -0.051974,  0.028408, -0.042594, -0.066716, -0.083513,  0.021399,\n",
       "        0.054435, -0.057351, -0.04807 ,  0.09534 ,  0.054126, -0.042042,\n",
       "       -0.006303, -0.021994,  0.102618,  0.121487, -0.032814, -0.066175,\n",
       "        0.071879,  0.050986,  0.015519,  0.101778, -0.017493, -0.0082  ,\n",
       "        0.11973 , -0.006143, -0.134806, -0.030183, -0.099897, -0.054662,\n",
       "        0.030468,  0.053002,  0.052606, -0.003097, -0.079577,  0.055978,\n",
       "        0.034838,  0.036461, -0.056506,  0.001546,  0.023694, -0.061339,\n",
       "       -0.030392,  0.028833,  0.044646,  0.044805,  0.035043,  0.089791,\n",
       "       -0.041356, -0.034887,  0.022974, -0.025776,  0.011678, -0.048851,\n",
       "        0.050003,  0.075171, -0.077506,  0.061281,  0.018234,  0.034924,\n",
       "       -0.054486, -0.023148,  0.04967 , -0.037797,  0.004899,  0.059524,\n",
       "        0.051093,  0.060388,  0.077362,  0.119049, -0.003686, -0.078293,\n",
       "        0.024893, -0.028049, -0.012109, -0.068692,  0.108549, -0.072499,\n",
       "        0.011194,  0.106088,  0.020271,  0.064712, -0.011894, -0.040882,\n",
       "       -0.118573,  0.051815,  0.042526,  0.032066,  0.186122,  0.027743,\n",
       "        0.06652 ,  0.105785, -0.024183, -0.014505, -0.002989,  0.085211,\n",
       "        0.152362,  0.089936,  0.007371,  0.135352, -0.131279, -0.181871,\n",
       "        0.065822,  0.05445 ,  0.206425,  0.025386,  0.000896,  0.013236,\n",
       "       -0.009589,  0.072919, -0.061631,  0.087747, -0.105101, -0.114079,\n",
       "       -0.024143,  0.011615, -0.032534, -0.094159,  0.125836, -0.158895,\n",
       "       -0.015744,  0.039135, -0.085246,  0.075489,  0.080253,  0.075218,\n",
       "       -0.090769,  0.130184,  0.022855,  0.083172,  0.034794,  0.0518  ,\n",
       "        0.066524, -0.004649, -0.035322,  0.113644, -0.037286, -0.00264 ,\n",
       "       -0.001534, -0.052365,  0.026423,  0.018602, -0.01582 ,  0.016643,\n",
       "       -0.153259,  0.060309, -0.09268 , -0.029055,  0.087337, -0.07463 ,\n",
       "        0.052572,  0.121729,  0.037575,  0.086311, -0.080767,  0.065263,\n",
       "        0.051773, -0.116826,  0.186747, -0.075001, -0.037817,  0.08585 ,\n",
       "       -0.095529,  0.126134])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using gensim to load the word embeddings doesn't seem to work because of formating (also the files are really big)\n",
    "# =>correct formating is another approach to try!\n",
    "#wordvecs = gensim.models.KeyedVectors.load_word2vec_format('in.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize document-query embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to represent a Document D with word embeddings\n",
    "Here D is the centroid of all the normalized vectors for the words in the document serving as a single embedding for the whole document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute a centroid of word embeddings\n",
    "To find the centroid, one computes the (arithmetic) mean of the points' positions separately for each dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing a Documents-Query Representation with PCA\n",
    "A  two  dimensional  PCA  projection  of  the  200-\n",
    "dimensional embeddings. Relevant documents are yellow, irrel-\n",
    "evant documents are grey, and the query is blue.  To visualize\n",
    "the  results  of  multiple  queries  at  once,  before  dimensionality\n",
    "reduction we centre query vectors at the origin and represent\n",
    "documents as the difference between the document vector and\n",
    "its query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Document-Centroid\n",
    "\n",
    "#A function to compute the euclidean norm of a vector\n",
    "def euclid_norm(v):\n",
    "    norm = 0\n",
    "    for i in v:\n",
    "        norm += i**2\n",
    "    return np.sqrt(norm)\n",
    "\n",
    "#A function to compute the absolute/manhattan norm of a vector\n",
    "def abs_norm(v):\n",
    "    norm = 0\n",
    "    for i in v:\n",
    "        norm += abs(i)\n",
    "    return norm\n",
    "\n",
    "#Compute the centroid of the word-vectors of a Document to get a Document-Vector\n",
    "def compute_centroid(D):\n",
    "    D_N = len(D)\n",
    "    centroid = np.zeros(len(D[0]))\n",
    "    for d in D:\n",
    "        centroid += d / euclid_norm(d)\n",
    "    return centroid/D_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute the document vectors by computing the centroid of the word-vectors of the document, yielding the data structure: doc_centroids[doc][entry]\n",
    "doc_centroids = []\n",
    "for embed in doc_embeds:\n",
    "    doc_centroids.append(compute_centroid(embed))\n",
    "#To test the result, print the length of the first document vector (each vector should have 200 entries)\n",
    "len(doc_centroids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "docs_query = copy.deepcopy(doc_centroids)\n",
    "for i in query_embeds[0]:\n",
    "    docs_query.append(i)\n",
    "len(docs_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a PCA-Object to project the 200-dim. Document-Vector into a 2-dim. Vector\n",
    "pca = PCA(n_components=2)\n",
    "doc_pca = pca.fit_transform(docs_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01539ef1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now we can visualize the Documents in a 2-dim. Space, for human interpretability\n",
    "pyplot.scatter(list(doc_pca[:, 0]), list(doc_pca[:, 1]))\n",
    "doc_title = ['Cambridge', 'Oxford', 'Giraffes', 'Giraffes_switched', 'Cambridge_switched', 'query1', 'query2']\n",
    "for idx, doc in enumerate(doc_title):\n",
    "    pyplot.annotate(doc, xy=(doc_pca[idx,0], doc_pca[idx,1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute DESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute DESM-Measure for a query and all documents (see paper for the DESM-formula)\n",
    "def desm(Q, D):\n",
    "    #Denominator\n",
    "    Q_N = len(Q)\n",
    "    #Sum of cosine similarities between query term and document\n",
    "    Q_cosine = 0\n",
    "    for q in Q:\n",
    "        Q_cosine += q.dot(D) / euclid_norm(q) * euclid_norm(D)\n",
    "    return Q_cosine/Q_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Cambridge', -0.0504932469512819),\n",
       " ('Oxford', -0.0545077283965729),\n",
       " ('Cambridge_switched', -0.056444004499691246),\n",
       " ('Giraffes_switched', -0.07427110128487868),\n",
       " ('Giraffes', -0.07809950457409828)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Helper-Function for the sorting of the DESM-Scores\n",
    "def sort_help(elem):\n",
    "    return elem[1]\n",
    "\n",
    "#Create list of tuples for each Document-Title & the DESM-Score of the Query & the Document\n",
    "ranking = [(doc_title[idx], desm(query_embeds[0], doc_centroids[idx])) for idx in range(len(doc_centroids))]\n",
    "\n",
    "#Sort the DESM-Score, to show the most relevant document on top of the list\n",
    "ranking.sort(key=sort_help, reverse=True)\n",
    "ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see that for the query \"cambridge\" the document about cambridge is the most relevant.\n",
    "This is followed by the document about oxford, which is also an article about a univerity.\n",
    "More interestingly, the article about cambridge, where the word \"cambridge\" was substituted by the word \"giraffe\" is next relevant even though it doesn't contains the word cambridge, while the document about giraffes where the word \"giraffe\" is substituded by the word \"cambridge\" has a much worse score. This makes sense because the document is about giraffes and not about cambridge. Hence word-stuffing-trick has way less impact on the DESM-ranking then it would have on a TFIDF-Ranking (because it would count the word cambridge which would give it much more relevance).\n",
    "As expected, the document about Giraffes has the least relevance in the ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
